자연어 처리 프로젝트

기본 구조 이해 (1~5)
https://dacon.io/competitions/official/235670/codeshare/1801?page=1&dtype=recent

자연어 처리
1. 텍스트 분류 (Text Classification) -> 문장이나 문서를 분류
2. 감성 분석 (Sentimental Analysis) -> 긍/부정 등 다양한 감정 판단
3. 내용 요약 (Text Summarization)  -> 추출 요약(중요한 문장 뽑아내 요약문) or 생성 요약(요약문을 새롭게 생성)
4. 기계 번역 (Machine Translation) -> 번역(의미론적인(Semantic) 부분까지 번역
5. 챗본(Chat Bot)

<< 처리 과정 >>
Preprocessing
불용어 제거(Stopwords removing), 형태소 분석(Stemming), 표제어 추출(Lemmatization)


Vectorization
One-hot Encoding, Count vectorization, Tfidf, Padding


Embedding
[(단어수준)Word2vec, Glove, Fasttext , Swivel ] [(문장 수준)Doc2vec] 등..


Modeling
GRU, LSTM, Attention


-----------------------------------------------------------------------------------------------------------
((((((((((((((((20220201)))))))))))))))))))))))  복습 1/

/////////////////////////////
konlpy 에 대해
https://mr-doosun.tistory.com/22
////////////////////////////
형태소 분석의 이유 : 
1. 주로 형태소 단위로 의미있는 단어 를 가져가고 싶거나 
2. 품사 태깅을 통해 형용사나 명사를 추출하고 싶을 때 많이 이용하게 됩니다.

** 형태소 분석은 어쩌면 모델링보다 성능에 더 중요한 영향을 미치는 아주 중요한 과정.
 시간이 허락한다면 다양한 형태소 분석기를 사용하여 결과를 비교하는 것을 추천**

Mecab: 굉장히 속도가 빠르면서도 좋은 분석 결과를 보여준다.
Komoran: 댓글과 같이 정제되지 않은 글에 대해서 먼저 사용해보면 좋다.(오탈자를 어느정도 고려해준다.)
Kkma: 분석 시간이 오래걸리기 때문에 잘 이용하지 않게 된다.
Okt: 품사 태깅 결과를 Noun, Verb등 알아보기 쉽게 반환해준다.
khaiii: 카카오에서 가장 최근에 공개한 분석기, 성능이 좋다고 알려져 있으며 다양한 실험이 필요하다.

 
어간 추출(Stemming) and 표제어 추출(Lemmatization)  (https://wikidocs.net/21707)
: 정규화 기법 중 코퍼스에 있는 단어의 개수를 줄일 수 있는 기법
이 두 작업이 갖고 있는 의미는 눈으로 봤을 때는 서로 다른 단어들이지만, 
하나의 단어로 일반화시킬 수 있다면 하나의 단어로 일반화시켜서 문서 내의 단어 수를 줄이겠다는 것

표제어(Lemma) : 사전에 등록된 단어

파싱(parsing) : 파싱은 어떤 페이지(문서, html 등)에서 내가 원하는 데이터를 특정 패턴이나 순서로 추출해 가공하는 것을 말한다. 
이러한 파서(parser) 역할을 하는 컴퓨터가 구문 트리(parse tree)로 재구성하는 구문 분석 과정

어간(Stem)을 추출하는 작업을 어간 추출(stemming)하는 과정



/////////////////////////////
 re.sub() 함수
 https://clolee.tistory.com/17
////////////////////////////



re.sub(pattern, repl, string, count=0, flags=0)   # count는 음수가 아닌 정수, count가 0 또는 생략이면 찾은 문자열을 모두 치환.


re.sub('패턴', '바꿀문자열', '문자열', 바꿀횟수)

 + 정규식 사용해 특수문자 제거 : https://www.delftstack.com/ko/howto/python/remove-special-characters-from-string-python/
                                         https://www.fun-coding.org/DS&AL3-4.html
ex)

import re

string = "Hey! What's up bro?"
new_string = re.sub(r"[^a-zA-Z0-9]","",string)  #문자(a-z,A-z, 숫자(0-9)가 아닌것(^)을  => "" (무)로 치환 => 특수문자 삭제
print(new_string)



///////////////
.split()

split 함수는 a.split()처럼 괄호 안에 아무 값도 넣어 주지 않으면 공백(스페이스, 탭, 엔터 등)을 기준으로 문자열을 나누어 준다. 
만약 b.split(':')처럼 괄호 안에 특정 값이 있을 경우에는 괄호 안의 값을 구분자로 해서 문자열을 나누어 준다.


” “.join( list ) :  리스트 형태의 각각의 문자열을 " " (공백)을 사이에 두고 붙임 ==>리스트에서 문자열으로  
==> 리스트 내에 정수는 못붙임!!!!
///////////////



////////////////
불용어 제거
////////////////
import re
tokenizer = Okt()
def text_preprocessing(text,tokenizer):
    
    stopwords = ['을', '를', '이', '가', '은', '는']    <----- 불용어
    
    txt = re.sub('[^가-힣a-z]', ' ', text)     <------- re.sub 이용해 영어소문자,한글을 제외한 모든 문자를 제거 (공백으로 치환)
    token = tokenizer.morphs(txt)
    token = [t for t in token if t not in stopwords]   <---- 불용어 아닌것만 token 리스트에 저장


==>
print(example_pre)
['이번', '에', '새롭게', '개봉', '한', '영화', '의', '배우', '들', '모두', '훌륭한', '연기력', '과', '아름다운', '목소리', '갖고', '있어']


////////////////////////
one- hot Encoding 하기  : https://dacon.io/competitions/official/235670/codeshare/1841?page=1&dtype=recent
//////////////////////


keras 이용

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import to_categorical

tokens : 리스트 형태로, 전처리 되어 형태소 단위로 분리(띄어쓰기) 된 문장열들이 리스트형태로 하나하니씩 저장

Tokenizer와 to_categorical 이용
t = Tokenizer
t.fit_on_tokens(tokens) : 각 토큰에게 고유의 정수 부여 => 하나하나씩
=> 이 내용은 t.word_index 에 사전형태로 저장된다.

정수로 표시한 문장 = t.text_to_sequences(tokens)[몇번째 문장] : 해당 문장을 부여된 정수로 표시 

one hot encoding 된 2차원 list = to_categorical(정수로 표시한 문장)   : one-hot encoding 처리

*****하지만 이 방식은 vocabulary 크기가 커짐에 따라 많은 공간을 차지하게 되고 벡터가 굉장히 sparse해지기 때문에 모델에게 좋은 특성을 알려주지 못하는 경우가 대부분


////////////////////////
sklearn Vectorizer 사용
////////////////////////

from sklearn.feature_extraction.text import (원하는 Vectorizer)

vectorizer = 원하는 Vectorizer()  #이름 복사
vectors = vectorizer.fit_transform(tokens)    # tokens는 여러개의 문장이여야 함

이후 index화 된 token들 확인
print(vectorizer.get_feature_names())

벡터 변환된 문장을 array 형태로 확인
print(vectors.toarray())
=> 출력 형태 [ [문장 1의 벡터] [문장2의 벡터] ...... ]



//////////////////////////////
padding
////////////////////////

 Padding
우리가 사용하던 모델들은 DataFrame 형식의 row별 동일한 colum수를 갖는데
 NLP에서는 row별(문장별) colum(토큰의 개수)가 같지 않아도 되는건가? 
가변 길이의 문장들을 입력으로 넣어도 된다고?

가변 길이의 입력을 받는 모형들이 존재하지만 아
쉽게도 기본적으로는 문장의 길이를 동일하게 맞춰주어야 한다
문장의 길이를 맞춰주기 위해 부족한 길이만큼 0을 채워넣게 되는데 , 
이것을 Padding라고 부른다

from tensorflow.keras.preprocessing.sequence import pad_sequences

vocab_size = len(tokenizer.word_index) + 1 #모델에 알려줄 vocabulary의 크기 계산
X_train = pad_sequences(train_X_seq (정수 index로 표현된 문장) , maxlen = max_len) #설정한 문장의 최대 길이만큼 padding



((((((((((((((((((((((((((((((((20220202)))))))))))))))))))))))))))))))))))))))))))))


///////////////////////
파이썬 scipy 희소생렬 : https://radish-greens.tistory.com/1
////////////////////////

2|4|4|0
0|0|1|1
0|0|0|5

행렬을 COO matrix로 표현

from scipy.sparse import coo_matrix

row = [0, 0, 0, 1, 2] # 행 인덱스를 담은 리스트
col = [0, 1, 2, 2, 3] # 열 인덱스를 담은 리스트
data = [2, 4, 2, 1, 5] # 원소 값을 담은 리스트

m = coo_matrix((data, (row, col)))
m

 0이 아닌 원소 값 5개의 행 인덱스를 row에, 열 인덱스는 col에 담았습니다. 이를 scipy.sparse.coo_matrix에 넘겨주면 희소행렬이 만들어진다.

이 구조는 sklearn의 Vectorizer를 통해 .fit_transform(tokens)를 통해 나온 행렬을 이해하는데 사용
=> 저 값을 .toarray() 해주면 우리가 아는 행렬형태로 볼 수 있음( 2D array형태로)



//////////////////////
word2vec : https://dreamgonfly.github.io/blog/word2vec-explained/
               https://dacon.io/competitions/official/235670/codeshare/1892?page=1&dtype=recent
               http://doc.mindscale.kr/km/unstructured/11.html (자세하게 잘되어있음)
///////////////////////


///////////////////////
파이썬 for in enumerate()   :  https://www.daleseo.com/python-enumerate/
///////////////////////

튜플형 파일 접근




((((((((((((((((((((((((((((((((((((((20220203))))))))))))))))))))))))))))))))))))))))))))))))))))))))))

//////////////////////
open()으로 txt 파일 접근 후, 문장단위로 접근하는 간단한 방법
////////////////////

text.txt
내에는

다음과 같은 정보가 담겨 있다.

테스트 입니다.
테스트 2 입니다.


------------------------------------------
t= open('test.txt','r',encoding="utf8")
print(t)
for line in t:
    print(line)
-----------------------------------------------
<_io.TextIOWrapper name='test.txt' mode='r' encoding='utf8'>
테스트 입니다.

테스트 2 입니다.
------------------------------------------------
<결론>
이전에 배웠을 때에는 lines = t.readlines() 를 통해 txt 내의 문장을 띄어쓰기 를 기준으로 분리하여, list 형태로 저장한 다음, 이 lines를 for 문을 이용해 for line in lines: "line을 다루는 코드" => 주의! 각 list내의 원소의 오른쪽 끝에는 \n을 포함하고 있으므로 print를 하거나 할 때, end=""를 해주어야 한다.

형식으로 접근하였지만, 지금 코드를 보니 그냥 open 한 것이, for문의 저장소 역할을 해 낼 수 있음을 볼 수 있다. ==> 옛날에 range(숫자) 만으로도 for 문의 저장소 역할을 했던 것과 유사하다!

이 for 문을 통해 txt 내의 문장을 각각 접근 가능함을 확인하였고, print에서 end=""를 해줄 필요가 없는 것을 보아 문장 끝에 \n을 포함하고 있지 않음을 알 수 있다.




//////////////////////
asarray() 함수  : https://studymake.tistory.com/406
///////////////////////

arr : 기본 array
할당값 = asarray(arr)   :  arr를 참조  => arr값을 바꾸면 이 할당값 내부도 변한다.
할당값 = array(arr)      :  arr를 복사하여 전달 =>  arr값을 바꾸어도 안변함



(((((((((((((((((((((((((20220204))))))))))))))))))))))))))))))))))))))))

/////////////////////////
Dropout()                      : https://stackoverflow.com/questions/50393666/how-to-understand-spatialdropout1d-and-when-to-use-it
SpatialDropout1D() 차이점
/////////////////////////

To make it simple, I would first note that so-called feature maps (1D, 2D, etc.) is our regular channels. Let's look at examples:

Dropout(): Let's define 2D input: [[1, 1, 1], [2, 2, 2]]. Dropout will consider every element independently, and may result in something like [[1, 0, 1], [0, 2, 2]]

SpatialDropout1D(): In this case result will look like [[1, 0, 1], [2, 0, 2]]. Notice that 2nd element was zeroed along all channels.

저기 내부에 0.2(20%로 0으로 떨굼) 등과 같은 rate 집어넣음

//////////////////////
Regularization  : https://wdprogrammer.tistory.com/33
///////////////////////
kernel_regularizer = regularizers.l2(0.001)
regularizers.l2(0.001) : 가중치 행렬의 모든 원소를 제곱하고 0.001을 곱하여 네트워크의 전체 손실에 더해진다는 의미, 이 규제(패널티)는 훈련할 때만 추가됨


//////////////////////
optimizer : https://onevision.tistory.com/entry/Optimizer-%EC%9D%98-%EC%A2%85%EB%A5%98%EC%99%80-%ED%8A%B9%EC%84%B1-Momentum-RMSProp-Adam
//////////////////////


