자연어 처리 프로젝트

기본 구조 이해 (1~5)
https://dacon.io/competitions/official/235670/codeshare/1801?page=1&dtype=recent

자연어 처리
1. 텍스트 분류 (Text Classification) -> 문장이나 문서를 분류
2. 감성 분석 (Sentimental Analysis) -> 긍/부정 등 다양한 감정 판단
3. 내용 요약 (Text Summarization)  -> 추출 요약(중요한 문장 뽑아내 요약문) or 생성 요약(요약문을 새롭게 생성)
4. 기계 번역 (Machine Translation) -> 번역(의미론적인(Semantic) 부분까지 번역
5. 챗본(Chat Bot)

<< 처리 과정 >>
Preprocessing
불용어 제거(Stopwords removing), 형태소 분석(Stemming), 표제어 추출(Lemmatization)


Vectorization
One-hot Encoding, Count vectorization, Tfidf, Padding


Embedding
[(단어수준)Word2vec, Glove, Fasttext , Swivel ] [(문장 수준)Doc2vec] 등..


Modeling
GRU, LSTM, Attention


-----------------------------------------------------------------------------------------------------------
((((((((((((((((20220201)))))))))))))))))))))))

/////////////////////////////
konlpy 에 대해
https://mr-doosun.tistory.com/22
////////////////////////////
형태소 분석의 이유 : 
1. 주로 형태소 단위로 의미있는 단어 를 가져가고 싶거나 
2. 품사 태깅을 통해 형용사나 명사를 추출하고 싶을 때 많이 이용하게 됩니다.

** 형태소 분석은 어쩌면 모델링보다 성능에 더 중요한 영향을 미치는 아주 중요한 과정.
 시간이 허락한다면 다양한 형태소 분석기를 사용하여 결과를 비교하는 것을 추천**

Mecab: 굉장히 속도가 빠르면서도 좋은 분석 결과를 보여준다.
Komoran: 댓글과 같이 정제되지 않은 글에 대해서 먼저 사용해보면 좋다.(오탈자를 어느정도 고려해준다.)
Kkma: 분석 시간이 오래걸리기 때문에 잘 이용하지 않게 된다.
Okt: 품사 태깅 결과를 Noun, Verb등 알아보기 쉽게 반환해준다.
khaiii: 카카오에서 가장 최근에 공개한 분석기, 성능이 좋다고 알려져 있으며 다양한 실험이 필요하다.

 
어간 추출(Stemming) and 표제어 추출(Lemmatization)  (https://wikidocs.net/21707)
: 정규화 기법 중 코퍼스에 있는 단어의 개수를 줄일 수 있는 기법
이 두 작업이 갖고 있는 의미는 눈으로 봤을 때는 서로 다른 단어들이지만, 
하나의 단어로 일반화시킬 수 있다면 하나의 단어로 일반화시켜서 문서 내의 단어 수를 줄이겠다는 것

표제어(Lemma) : 사전에 등록된 단어

파싱(parsing) : 파싱은 어떤 페이지(문서, html 등)에서 내가 원하는 데이터를 특정 패턴이나 순서로 추출해 가공하는 것을 말한다. 
이러한 파서(parser) 역할을 하는 컴퓨터가 구문 트리(parse tree)로 재구성하는 구문 분석 과정

어간(Stem)을 추출하는 작업을 어간 추출(stemming)하는 과정



/////////////////////////////
 re.sub() 함수
 https://clolee.tistory.com/17
////////////////////////////



re.sub(pattern, repl, string, count=0, flags=0)

re.sub('패턴', '바꿀문자열', '문자열', 바꿀횟수)

 + 정규식 사용해 특수문자 제거 : https://www.delftstack.com/ko/howto/python/remove-special-characters-from-string-python/

ex)

import re

string = "Hey! What's up bro?"
new_string = re.sub(r"[^a-zA-Z0-9]","",string)
print(new_string)


count는 음수가 아닌 정수, count가 0 또는 생략이면 찾은 문자열을 모두 치환.


///////////////
.split()

split 함수는 a.split()처럼 괄호 안에 아무 값도 넣어 주지 않으면 공백(스페이스, 탭, 엔터 등)을 기준으로 문자열을 나누어 준다. 
만약 b.split(':')처럼 괄호 안에 특정 값이 있을 경우에는 괄호 안의 값을 구분자로 해서 문자열을 나누어 준다.


” “.join( list ) : 리스트에서 문자열으로

///////////////



////////////////
불용어 제거
////////////////
import re
tokenizer = Okt()
def text_preprocessing(text,tokenizer):
    
    stopwords = ['을', '를', '이', '가', '은', '는']    <----- 불용어
    
    txt = re.sub('[^가-힣a-z]', ' ', text)     <------- re.sub 이용해 영어소문자,한글을 제외한 모든 문자를 제거 (공백으로 치환)
    token = tokenizer.morphs(txt)
    token = [t for t in token if t not in stopwords]   <---- 불용어 아닌것만 token 리스트에 저장


==>
print(example_pre)
['이번', '에', '새롭게', '개봉', '한', '영화', '의', '배우', '들', '모두', '훌륭한', '연기력', '과', '아름다운', '목소리', '갖고', '있어']


////////////////////////
one- hot Encoding 하기  : https://dacon.io/competitions/official/235670/codeshare/1841?page=1&dtype=recent
//////////////////////


keras 이용

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import to_categorical

tokens : 리스트 형태로, 전처리 되어 형태소 단위로 분리(띄어쓰기) 된 문장열들이 리스트형태로 하나하니씩 저장

Tokenizer와 to_categorical 이용
t = Tokenizer
t.fit_on_tokens(tokens) : 각 토큰에게 고유의 정수 부여 => 하나하나씩
=> 이 내용은 t.word_index 에 사전형태로 저장된다.

정수로 표시한 문장 = t.text_to_sequences(tokens)[몇번째 문장] : 해당 문장을 부여된 정수로 표시 

one hot encoding 된 2차원 list = to_categorical(정수로 표시한 문장)   : one-hot encoding 처리

*****하지만 이 방식은 vocabulary 크기가 커짐에 따라 많은 공간을 차지하게 되고 벡터가 굉장히 sparse해지기 때문에 모델에게 좋은 특성을 알려주지 못하는 경우가 대부분


////////////////////////
sklearn Vectorizer 사용
////////////////////////

from sklearn.feature_extraction.text import (원하는 Vectorizer)

vectorizer = 원하는 Vectorizer()  #이름 복사
vectors = vectorizer.fit_transform(tokens)    # tokens는 여러개의 문장이여야 함

이후 index화 된 token들 확인
print(vectorizer.get_feature_names())

벡터 변환된 문장을 array 형태로 확인
print(vectors.toarray())
=> 출력 형태 [ [문장 1의 벡터] [문장2의 벡터] ...... ]



//////////////////////////////
padding
////////////////////////

 Padding
우리가 사용하던 모델들은 DataFrame 형식의 row별 동일한 colum수를 갖는데
 NLP에서는 row별(문장별) colum(토큰의 개수)가 같지 않아도 되는건가? 
가변 길이의 문장들을 입력으로 넣어도 된다고?

가변 길이의 입력을 받는 모형들이 존재하지만 아
쉽게도 기본적으로는 문장의 길이를 동일하게 맞춰주어야 한다
문장의 길이를 맞춰주기 위해 부족한 길이만큼 0을 채워넣게 되는데 , 
이것을 Padding라고 부른다

from tensorflow.keras.preprocessing.sequence import pad_sequences

vocab_size = len(tokenizer.word_index) + 1 #모델에 알려줄 vocabulary의 크기 계산
X_train = pad_sequences(train_X_seq (정수 index로 표현된 문장) , maxlen = max_len) #설정한 문장의 최대 길이만큼 padding



